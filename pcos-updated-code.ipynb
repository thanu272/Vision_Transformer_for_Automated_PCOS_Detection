{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-21T09:11:15.506386Z","iopub.status.busy":"2024-03-21T09:11:15.505389Z","iopub.status.idle":"2024-03-21T09:11:28.889457Z","shell.execute_reply":"2024-03-21T09:11:28.888228Z","shell.execute_reply.started":"2024-03-21T09:11:15.506339Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torchvision\n","from torchvision import datasets, transforms, models\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import wandb\n","from tqdm.notebook import tqdm\n","import torch.nn as nn\n","import timm"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:12:53.354275Z","iopub.status.busy":"2024-03-21T09:12:53.353870Z","iopub.status.idle":"2024-03-21T09:12:54.649120Z","shell.execute_reply":"2024-03-21T09:12:54.647715Z","shell.execute_reply.started":"2024-03-21T09:12:53.354245Z"},"trusted":true},"outputs":[],"source":["\n","data_dir = \"D:/Semester 6 Materials/DLSIP/PCOS_Clean/archive (4)/data\"\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n","\n","train_size = int(0.7 * len(dataset))\n","val_size = int(0.15 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n","\n","class_to_idx = dataset.class_to_idx\n","idx_to_class = {value: key for key, value in class_to_idx.items()}\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:13:44.787128Z","iopub.status.busy":"2024-03-21T09:13:44.786698Z","iopub.status.idle":"2024-03-21T09:13:45.140812Z","shell.execute_reply":"2024-03-21T09:13:45.139611Z","shell.execute_reply.started":"2024-03-21T09:13:44.787095Z"},"trusted":true},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","train_data_dir = \"D:/Semester 6 Materials/DLSIP/PCOS_Clean/archive (4)/data/train\"\n","test_data_dir = \"D:/Semester 6 Materials/DLSIP/PCOS_Clean/archive (4)/data/test\"\n","\n","train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n","\n","test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n","test_size = int(0.7 * len(test_dataset))\n","val_size = int(len(test_dataset)-test_size)\n","test_dataset,val_dataset = torch.utils.data.random_split(test_dataset,[test_size,val_size])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:13:55.225433Z","iopub.status.busy":"2024-03-21T09:13:55.224549Z","iopub.status.idle":"2024-03-21T09:13:55.230604Z","shell.execute_reply":"2024-03-21T09:13:55.229585Z","shell.execute_reply.started":"2024-03-21T09:13:55.225396Z"},"trusted":true},"outputs":[],"source":["class_to_idx = train_dataset.class_to_idx\n","idx_to_class = {value: key for key, value in class_to_idx.items()}"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:14:02.870983Z","iopub.status.busy":"2024-03-21T09:14:02.870191Z","iopub.status.idle":"2024-03-21T09:14:02.879366Z","shell.execute_reply":"2024-03-21T09:14:02.878176Z","shell.execute_reply.started":"2024-03-21T09:14:02.870945Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'infected': 0, 'notinfected': 1}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["class_to_idx"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:14:10.026422Z","iopub.status.busy":"2024-03-21T09:14:10.026015Z","iopub.status.idle":"2024-03-21T09:14:47.893564Z","shell.execute_reply":"2024-03-21T09:14:47.892524Z","shell.execute_reply.started":"2024-03-21T09:14:10.026391Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Dataset:\n","test: 781 samples\n","train: 1143 samples\n","\n","Validation Dataset:\n","train: 332 samples\n","test: 245 samples\n","\n","Test Dataset:\n","test: 536 samples\n","train: 809 samples\n"]}],"source":["def count_samples_per_class(dataset, idx_to_class):\n","    counts = {}\n","    for _, label in dataset:\n","        class_name = idx_to_class[label]\n","        counts[class_name] = counts.get(class_name, 0) + 1\n","    return counts\n","\n","\n","class_to_idx = dataset.class_to_idx\n","idx_to_class = {value: key for key, value in class_to_idx.items()}\n","\n","train_counts = count_samples_per_class(train_dataset, idx_to_class)\n","val_counts = count_samples_per_class(val_dataset, idx_to_class)\n","test_counts = count_samples_per_class(test_dataset, idx_to_class)\n","\n","print(\"Training Dataset:\")\n","for class_name, count in train_counts.items():\n","    print(f\"{class_name}: {count} samples\")\n","\n","print(\"\\nValidation Dataset:\")\n","for class_name, count in val_counts.items():\n","    print(f\"{class_name}: {count} samples\")\n","\n","print(\"\\nTest Dataset:\")\n","for class_name, count in test_counts.items():\n","    print(f\"{class_name}: {count} samples\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:14:52.296130Z","iopub.status.busy":"2024-03-21T09:14:52.295688Z","iopub.status.idle":"2024-03-21T09:15:31.495776Z","shell.execute_reply":"2024-03-21T09:15:31.494710Z","shell.execute_reply.started":"2024-03-21T09:14:52.296098Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet169_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet169_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["modifying VGG16...\n","modifying VGG19...\n","modifying InceptionV3...\n","modifying GoogLeNet...\n","modifying ResNet50...\n","modifying ResNet152...\n","modifying DenseNet121...\n","modifying DenseNet169...\n","modifying DenseNet201...\n","modifying MobileNetV2...\n","Adding ViT from timm.....\n"]}],"source":["def modify_last_layer(model):\n","    \n","    last_layer = list(model.children())[-1]\n","    \n","    \n","    if isinstance(last_layer, nn.modules.container.Sequential):\n","        num_ftrs = last_layer[-1].out_features\n","    else:\n","        num_ftrs = last_layer.out_features\n","    \n","   \n","    classification_head = nn.Sequential(\n","        nn.Linear(num_ftrs, 2048),\n","        nn.ReLU(),\n","        nn.Linear(2048, 1024),\n","        nn.ReLU(),\n","        nn.Linear(1024, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 256),\n","        nn.ReLU(),\n","        nn.Linear(256,64),\n","        nn.ReLU(),\n","        nn.Linear(64,4)\n","    )\n","\n","    classification_head = classification_head #create_classification_head(num_ftrs)\n","    \n","   \n","    \n","    \n","    model.classification_head = classification_head\n","\n","    \n","    original_forward = model.forward\n","\n","    def new_forward(x):\n","        x = original_forward(x)\n","        return model.classification_head(x)\n","\n","    model.forward = new_forward\n","    \n","    return model\n","\n","\n","\n","def get_vit_model(model_name):\n"," \n","    # Load the pre-trained Vision Transformer model\n","    model = timm.create_model(model_name, pretrained=True)\n","    num_ftrs = model.head.in_features\n","    # Replace the classification head\n","    model.head = nn.Sequential(\n","        nn.Linear(num_ftrs, 2048),\n","        nn.ReLU(),\n","        nn.Linear(2048, 1024),\n","        nn.ReLU(),\n","        nn.Linear(1024, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 256),\n","        nn.ReLU(),\n","        nn.Linear(256,64),\n","        nn.ReLU(),\n","        nn.Linear(64,2)\n","    )\n","    \n","    return model\n","\n","\n","models_and_names = {\n","    \"VGG16\": models.vgg16(pretrained=True),\n","    \"VGG19\": models.vgg19(pretrained=True),\n","    \"InceptionV3\": models.inception_v3(pretrained=True),\n","    \"GoogLeNet\": models.googlenet(pretrained=True),\n","    \"ResNet50\": models.resnet50(pretrained=True),\n","    \"ResNet152\": models.resnet152(pretrained=True),\n","    \"DenseNet121\": models.densenet121(pretrained=True),\n","    \"DenseNet169\": models.densenet169(pretrained=True),\n","    \"DenseNet201\": models.densenet201(pretrained=True),\n","    \"MobileNetV2\": models.mobilenet_v2(pretrained=True),\n","    \n","}\n","\n","\n","results = {}\n","for model_name, model in models_and_names.items():\n","    print(f\"modifying {model_name}...\")\n","    \n","    \n","    for param in model.parameters():\n","        param.requires_grad = True\n","    \n","    model = modify_last_layer(model)\n","    \n","print(\"Adding ViT from timm.....\")\n","\n","models_and_names[\"ViT-B-16\"] = get_vit_model(\"vit_base_patch16_224\")\n","models_and_names[\"ViT-L-16\"] = get_vit_model(\"vit_large_patch16_224\")\n","#models_and_names[\"ViT-L-32\"] = get_vit_model(\"vit_large_patch32_224\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:16:58.805151Z","iopub.status.busy":"2024-03-21T09:16:58.802794Z","iopub.status.idle":"2024-03-21T09:16:58.815668Z","shell.execute_reply":"2024-03-21T09:16:58.814275Z","shell.execute_reply.started":"2024-03-21T09:16:58.805094Z"},"trusted":true},"outputs":[],"source":["import random\n","import string\n","\n","def generate_random_character():\n","    return random.choice(string.ascii_letters)\n","\n","\n","def generate_random_string(length=1):\n","    return ''.join(random.choice(string.ascii_letters) for _ in range(length))\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:17:39.720145Z","iopub.status.busy":"2024-03-21T09:17:39.719491Z","iopub.status.idle":"2024-03-21T09:17:39.770028Z","shell.execute_reply":"2024-03-21T09:17:39.768675Z","shell.execute_reply.started":"2024-03-21T09:17:39.720111Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def test(model,testloader,model_name):\n","    # Test\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(testloader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = outputs.max(1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Plot confusion matrix\n","    confusion_mtx = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.show()\n","    plt.savefig(f\"figures/confusionmatrix/{model_name}.png\")\n","    \n","    return confusion_mtx\n","\n","\n","def calculate_metrics_from_confusion_matrix(confusion_matrix,model_name):\n","  \n","    num_classes = confusion_matrix.shape[0]\n","    overall_precision = 0\n","    overall_recall = 0\n","    overall_f1 = 0\n","    correct = np.trace(confusion_matrix)  # Sum of diagonal elements\n","    total = np.sum(confusion_matrix)\n","    \n","    for c in range(num_classes):\n","        TP = confusion_matrix[c, c]\n","        FP = np.sum(confusion_matrix[:, c]) - TP\n","        FN = np.sum(confusion_matrix[c, :]) - TP\n","        TN = total - TP - FP - FN\n","        \n","        precision = TP / (TP + FP) if TP + FP != 0 else 0\n","        recall = TP / (TP + FN) if TP + FN != 0 else 0\n","        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n","        \n","        overall_precision += precision\n","        overall_recall += recall\n","        overall_f1 += f1\n","    \n","    # Average the metrics across all classes\n","    overall_precision /= num_classes\n","    overall_recall /= num_classes\n","    overall_f1 /= num_classes\n","    accuracy = correct / total\n","    \n","    # Print the metrics\n","    print(f\"Model: {model_name}\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Average Precision: {overall_precision:.4f}\")\n","    print(f\"Average Recall: {overall_recall:.4f}\")\n","    print(f\"Average F1 Score: {overall_f1:.4f}\")\n","    \n","    return {\n","        'accuracy': accuracy,\n","        'average_precision': overall_precision,\n","        'average_recall': overall_recall,\n","        'average_f1': overall_f1\n","    }\n","\n","import glob\n","import os\n","import copy\n","\n","# Function to train and evaluate a given model\n","def train_and_evaluate(model, model_name, train_dataset, val_dataset, test_dataset,batch_size=32, num_epochs=50, lr=0.001,verbose_freq=100,other_logs=None):\n","    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","    \n","    \n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    run_name = f\"{model_name}_{generate_random_string(10)}\"\n","    wandb.init(project='transfer-learning-PDMSAPSP-ALL-2', name = run_name)\n","   \n","    hyperparameters = {\n","        \"learning_rate\": lr,\n","        \"batch_size\": 32,\n","        \"epochs\": num_epochs\n","    }\n","    \n","    wandb.config.update(hyperparameters)\n","    wandb.log({\"model_name\": model_name})\n","    if other_logs is not None:\n","        wandb.log(other_logs)\n","\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    # Training\n","    num_epochs = num_epochs\n","    steps = 0\n","    best_model=0\n","    best_acc=0\n","    \n","    for epoch in tqdm(range(num_epochs)):\n","        model.train()\n","        for inputs, labels in tqdm(trainloader):\n","            steps += 1\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Log metrics and details every 250 steps\n","            if steps % verbose_freq == 0:\n","                wandb.log({\"loss\": loss.item()})\n","                # Checkpoint model\n","                \n","                \n","\n","                # Validation\n","                model.eval()\n","                val_loss = 0.0\n","                correct = 0\n","                total = 0\n","                with torch.no_grad():\n","                    for inputs, labels in tqdm(valloader):\n","                        inputs, labels = inputs.to(device), labels.to(device)\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","                        val_loss += loss.item()\n","                        _, predicted = outputs.max(1)\n","                        total += labels.size(0)\n","                        correct += predicted.eq(labels).sum().item()\n","                val_acc = 100* correct / total\n","                wandb.log({\"val acc\":  100* correct / total})\n","                print(correct,total)\n","                \n","                if best_acc<val_acc:\n","                    if os.path.exists(f'{run_name}_{best_model}.pth'):\n","                        os.remove(f'{run_name}_{best_model}.pth')\n","                    best_acc = val_acc\n","                    best_model = steps\n","                    torch.save(model.state_dict(), f'{run_name}_{best_model}.pth')\n","#                     wandb.save(f'{run_name}_{best_model}.pth')\n","                    wandb.log({\"best val acc\": best_acc})\n","\n","\n","                print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss / len(valloader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n","                \n","    \n","    #Delete all models except best model\n","    saved_models = glob.glob(\"./*pth\")\n","    best_model_name = f\"{run_name}_{best_model}.pth\"\n","    for model_ in saved_models:\n","        if  best_model_name != model_.split(\"/\")[-1]:\n","            os.remove(model_)\n","            \n","    \n","    #Test\n","    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","    best_model = copy.deepcopy(model)\n","    best_model.load_state_dict(torch.load(best_model_name))\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    best_model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(testloader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = best_model(inputs)\n","            _, predicted = outputs.max(1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Plot confusion matrix\n","    confusion_mtx = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues')\n","    plt.xlabel('Predicted')\n","    plt.title(f\"{model_name} Best Model Confusion Matrix\")\n","    plt.ylabel('True')\n","    \n","    plt.show()\n","    plt.savefig(f\"{model_name} CM.png\")\n","    wandb.save(f\"{model_name} CM.png\")\n","    \n","    metrics  = calculate_metrics_from_confusion_matrix(confusion_mtx,\"ResNet50\")\n","    wandb.log(metrics)  \n","    wandb.finish()\n","    \n","    return metrics,best_model\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:17:59.645043Z","iopub.status.busy":"2024-03-21T09:17:59.644370Z","iopub.status.idle":"2024-03-21T09:17:59.652580Z","shell.execute_reply":"2024-03-21T09:17:59.651036Z","shell.execute_reply.started":"2024-03-21T09:17:59.645011Z"},"trusted":true},"outputs":[{"data":{"text/plain":["dict_keys(['VGG16', 'VGG19', 'InceptionV3', 'GoogLeNet', 'ResNet50', 'ResNet152', 'DenseNet121', 'DenseNet169', 'DenseNet201', 'MobileNetV2', 'ViT-B-16', 'ViT-L-16'])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["models_and_names.keys()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from getpass import getpass\n","key = getpass(\"Enter key\")\n","wandb.login(key=key)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:18:06.180778Z","iopub.status.busy":"2024-03-21T09:18:06.180296Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]}],"source":["\n","metrics,best_model = train_and_evaluate(models_and_names[\"ViT-L-16\"],\"ViT-L-16\",train_dataset, val_dataset,test_dataset,lr=0.0001, num_epochs=10,other_logs={\"finetune_all\": \"True\"})\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["idx_to_class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_embeddings(model, dataloader):\n","    embeddings = []\n","    labels_list = []\n","\n","    def hook(module, input, output):\n","        embeddings.append(output.cpu().numpy())\n","\n","    handle = model.head[8].register_forward_hook(hook)\n","\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            _ = model(inputs)\n","            labels_list.extend(labels.cpu().numpy())\n","    \n","    handle.remove()\n","\n","    embeddings = np.concatenate(embeddings, axis=0)\n","    return embeddings, labels_list\n","\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = models_and_names[\"ViT L16\"]\n","model_path = \"/kaggle/working/ResNet152_oKCzhfWSKa_6900.pth\"\n","model.load_state_dict(torch.load(model_path))\n","model = model.to(device)\n","model.eval()\n","\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","\n","def visualize_tsne(embeddings, labels):\n","    tsne = TSNE(n_components=2, random_state=42)\n","    reduced = tsne.fit_transform(embeddings)\n","\n","    plt.figure(figsize=(12,10))\n","    for i, label in enumerate(np.unique(labels)):\n","        plt.scatter(reduced[labels == label, 0], reduced[labels == label, 1], label=idx_to_class[label])\n","        \n","    \n","    plt.grid(True)\n","    plt.xlabel(\"t-SNE component 1\", fontsize=14)\n","    plt.ylabel(\"t-SNE component 2\", fontsize=14)\n","    plt.title(\"t-SNE visualization of embeddings from Best ResNET152 Model\", fontsize=16)\n","    plt.legend(loc='upper right', fontsize=12)\n","    plt.tight_layout()\n","    \n","    plt.savefig(\"tsne_visualization_resnet152.png\", format=\"png\")\n","    plt.show()\n","\n","   \n","\n","# Extract embeddings and visualize\n","embeddings, labels = extract_embeddings(model, testloader)\n","visualize_tsne(embeddings, labels)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4546896,"sourceId":7772154,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
